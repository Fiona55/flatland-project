Nous avons essayÃ© diffÃ©rentes stratÃ©gies : 1+ğº, 1+ğ›ğº, algorithme gÃ©nÃ©tique, CMA, PSO (particle swarm optimisation). Finalement, nous avons obtenus des rÃ©sultats similaires pour deux dâ€™entre elles : PSO et algorithme gÃ©nÃ©tique. Les autres donnent des rÃ©sultats toujours plus mauvais peu importe les modifications. Les deux semblent robustes en donnant des rÃ©sultats similaires dans les tests cependant lâ€™algorithme PSO a nÃ©cessitÃ© beaucoup plus de temps Ã  optimiser. En effet, il comporte beaucoup plus de paramÃ¨tres et est donc trÃ¨s compliquÃ© Ã  optimiser. 
Il semble donc logique de prendre lâ€™algorithme le plus simple et comprÃ©hensible et nous avons choisi lâ€™algorithme gÃ©nÃ©tique.

Algorithme GÃ©nÃ©tique :
Lâ€™implÃ©mentation se fait en utilisant les fonctions de la bibliothÃ¨que pymoo pymoo.algorithms.soo.nonconvex.ga. Il faut donc installer le package Pymoo avant dâ€™utiliser lâ€™algorithme (pour tester les autres algorithmes implÃ©mentÃ©s, il faut aussi installer les packages cma et Pyswarm). 
On dÃ©finit ensuite un problÃ¨me dâ€™optimisation Ã  lâ€™aide de la classe ElementWiseProblem. Le nombre de variable du problÃ¨me est la longueur du tableau de solution x.

Cet algorithme a un trÃ¨s bon potentiel puisquâ€™il renvoie sans optimisation des hyper-paramÃ¨tres (paramÃ¨tres par dÃ©faut) une fitness de -56 sur le problÃ¨me Â«Â smallÂ Â».
Les paramÃ¨tres que nous avons modifiÃ©s pour amÃ©liorer les performances sont la taille de la population et la taille du rÃ©seau de neurones. 
Nous nâ€™avons pas changÃ© la stratÃ©gie dâ€™observation (TreeObservation) puisque la stratÃ©gie locale est obsolÃ¨te (mauvaise performante) et en augmentant le rayon, on peut quand mÃªme modifier la stratÃ©gie. 
Quand Ã  la policy choisie, nous avons gardÃ© le rÃ©seau de neurones sans modifier sa profondeur (rajouter des couches et rÃ©duire le nombre de neurons amÃ©liorait la performance de PSO et CMA mais pas de lâ€™algorithme gÃ©nÃ©tique). Lâ€™ajout dâ€™un dropout ou lâ€™activation softmax amÃ©liorait aussi lâ€™algorithme PSO mais pas lâ€™algorithme gÃ©nÃ©tique.
On se rend compte ici que ce modÃ¨le est simple mais efficace tandis que PSO nÃ©cessitait de nombreuses features additionnelles.

Les paramÃ¨tres retenus pour cette mÃ©thode sontÂ : pop_size = 20 (taille de la population) et hidsize1 = hidsize2 = 128. Augmenter la taille de la population ou rÃ©duire le nombre de neurones faisait stagner lâ€™algorithme Ã  une mauvaise fitness.

En optimisant les paramÃ¨tres, on obtient facilement une fitness de 0 pour le problÃ¨me Â«Â smallÂ Â». En passant Ã  lâ€™Ã©chelle du large, on peut continuer lâ€™optimisation des paramÃ¨tres. 
On peut tester le rÃ©sultat obtenu avec le fichier test.py, que lâ€™on a modifiÃ© pour faire 15 tests. Notre algorithme se rÃ©vÃ¨le trÃ¨s robuste puisque les 15 tests donnent une mean fit de -438 tandis que la plage de valeurs est comprise entre -435 et - 460.